
---

# ğŸ§  RAG Chatbot (PDF-based)

A **Retrieval-Augmented Generation (RAG) chatbot** that allows users to **upload PDF documents and ask questions about them**.
The system retrieves relevant context from uploaded PDFs and generates accurate answers using LLMs.

This project is built as a **full-stack application** with a FastAPI backend and a React frontend.

---

## ğŸš€ Features

* ğŸ“„ Upload PDF files from the frontend
* ğŸ” Extract, chunk, and embed PDF content
* ğŸ§  Semantic search using vector embeddings (FAISS)
* ğŸ¤– Answer questions using LLMs with retrieved context
* ğŸŒ REST API built with FastAPI
* âš›ï¸ Frontend built using React
* ğŸ”— CORS-enabled backend for frontend integration

---

## ğŸ—ï¸ Tech Stack

### Backend

* **FastAPI**
* **LangChain**
* **FAISS** (Vector Store)
* **Hugging Face Embeddings**
* **OpenRouter API** (LLM access)
* **Python**

### Frontend

* **React**
* **JavaScript**
* **CSS**
* **Fetch API**

---

## ğŸ“ Project Structure

### Backend (separate repository)

```
backend/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ loader.py
â”‚       â”œâ”€â”€ vectorstore.py
â”‚       â””â”€â”€ chain.py
â”‚â”€â”€ data/
â”‚   â””â”€â”€ pdfs/
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .env
```

### Frontend (this repository)

```
frontend/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx
â”‚   â”œâ”€â”€ App.css
â”‚   â””â”€â”€ assets/
â”‚       â””â”€â”€ attached-file.png
â”‚â”€â”€ public/
â”‚â”€â”€ package.json
â”‚â”€â”€ .gitignore
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/TS563/RAG-CHATBOT-FRONTEND.git
cd RAG-CHATBOT-FRONTEND
```

---

### 2ï¸âƒ£ Install Frontend Dependencies

```bash
npm install
```

---

### 3ï¸âƒ£ Start Frontend Server

```bash
npm run dev
```

Frontend will run on:

```
http://localhost:5173
```

---

## ğŸ”Œ Backend Connection

The frontend communicates with the backend via REST APIs.

### API Endpoints Used

* `POST /upload` â†’ Upload PDF file
* `POST /chat` â†’ Ask a question based on uploaded PDFs

> âš ï¸ Make sure the **backend server is running** before using the chatbot.

---

## ğŸ” Environment Variables (Recommended)

Although this is a **personal project**, it is recommended to use environment variables.

Example (frontend):

```
VITE_API_BASE_URL=http://127.0.0.1:8000
```

Then use it in code:

```js
fetch(`${import.meta.env.VITE_API_BASE_URL}/chat`)
```

ğŸš« Do **NOT** commit `.env` files.

---

## ğŸ§ª How It Works (RAG Flow)

1. User uploads a PDF
2. PDF is parsed and split into chunks
3. Chunks are converted into embeddings
4. Embeddings are stored in FAISS vector store
5. User asks a question
6. Relevant chunks are retrieved
7. LLM generates an answer using retrieved context

---

## ğŸ“Œ Current Limitations

* Only PDF files supported
* Vector store is in-memory (not persistent yet)
* Single-user workflow
* Basic UI (no chat history yet)

---

## ğŸ”® Future Improvements

* Persistent vector storage
* Chat history
* Multiple document support
* Authentication
* Streaming responses
* Deployment (Docker / Cloud)

---

## ğŸ‘¤ Author

**Tanish Sharma**
B.Tech (AI & ML)
Passionate about Backend, APIs, and Open Source

GitHub: [https://github.com/TS563](https://github.com/TS563)

---

## â­ Acknowledgements

* LangChain
* Hugging Face
* FastAPI
* OpenRouter
* Open Source Community

---

